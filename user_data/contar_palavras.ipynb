{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973d61fc",
   "metadata": {},
   "source": [
    "# Contador de Palavras com PySpark\n",
    "Este notebook demonstra como usar PySpark para contar palavras em arquivos de texto armazenados no HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c900b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc6be8",
   "metadata": {},
   "source": [
    "## Inicializando Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9daac5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum SparkContext anterior encontrado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/05 14:49:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/05 14:49:10 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark iniciado com sucesso!\n",
      "Spark Version: 3.5.0\n",
      "Spark Master: yarn\n",
      "App Name: contagem_palavras_lab6\n",
      "Default FS: hdfs://spark-master:9000\n"
     ]
    }
   ],
   "source": [
    "# Parar qualquer SparkContext existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"SparkContext anterior finalizado\")\n",
    "except NameError:\n",
    "    print(\"Nenhum SparkContext anterior encontrado\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao finalizar SparkContext: {e}\")\n",
    "\n",
    "# Inicializando Spark\n",
    "findspark.init(\"/usr/spark-3.5.0/\")\n",
    "\n",
    "# Configura√ß√£o para usar YARN e HDFS\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"contagem_palavras_lab6\")\n",
    "    .master(\"yarn\")  # Usar YARN agora que est√° funcionando\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://spark-master:9000/tmp/hive\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://spark-master:9000\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# criar um contexto de sess√£o do spark (cria um \"programa\")\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"‚úÖ Spark iniciado com sucesso!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {sc.master}\")\n",
    "print(f\"App Name: {sc.appName}\")\n",
    "print(f\"Default FS: {sc._jsc.hadoopConfiguration().get('fs.defaultFS')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ddf5c",
   "metadata": {},
   "source": [
    "## Fun√ß√£o Principal - Contagem de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2ffa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando an√°lise de palavras...\n",
      "Lendo arquivos do HDFS...\n",
      "Lendo arquivos: hdfs://spark-master:9000/datasets/*.txt\n",
      "N√∫mero de parti√ß√µes: 5\n",
      "Processando texto...\n",
      "Contando palavras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Estat√≠sticas:\n",
      "Total de palavras processadas: 614,956\n",
      "Palavras √∫nicas: 33,976\n",
      "\n",
      "üîù Top 15 palavras mais comuns:\n",
      "   1. the             : 36,279\n",
      "   2. and             : 21,402\n",
      "   3. that            : 10,899\n",
      "   4. was             :  9,166\n",
      "   5. his             :  8,155\n",
      "   6. her             :  7,340\n",
      "   7. you             :  7,174\n",
      "   8. with            :  6,804\n",
      "   9. not             :  6,489\n",
      "  10. had             :  6,323\n",
      "  11. for             :  6,026\n",
      "  12. but             :  5,830\n",
      "  13. she             :  5,726\n",
      "  14. have            :  4,254\n",
      "  15. him             :  4,168\n",
      "\n",
      "üíæ Salvando resultado em: hdfs://spark-master:9000/datasets_processed/word_count_result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultado salvo com sucesso no HDFS!\n",
      "\n",
      "üìÅ Verificando arquivos salvos:\n",
      "Linhas salvas: 33976\n",
      "\n",
      "üéâ An√°lise conclu√≠da com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Iniciando an√°lise de palavras...\")\n",
    "    \n",
    "    try:\n",
    "        # Lendo arquivos do HDFS agora que est√° funcionando\n",
    "        print(\"Lendo arquivos do HDFS...\")\n",
    "        hdfs_path = \"hdfs://spark-master:9000/datasets/*.txt\"\n",
    "        \n",
    "        # Leitura dos arquivos de texto pelo programa spark\n",
    "        print(f\"Lendo arquivos: {hdfs_path}\")\n",
    "        text_files = sc.textFile(hdfs_path)\n",
    "        \n",
    "        # Verificar se conseguiu ler algum arquivo\n",
    "        file_count = text_files.getNumPartitions()\n",
    "        print(f\"N√∫mero de parti√ß√µes: {file_count}\")\n",
    "        \n",
    "        # Transforma√ß√£o: dividir linhas em palavras e limpar\n",
    "        print(\"Processando texto...\")\n",
    "        words = (text_files\n",
    "                .flatMap(lambda line: line.split())\n",
    "                .filter(lambda word: len(word) > 2)  # Filtrar palavras muito pequenas\n",
    "                .map(lambda word: ''.join(c.lower() for c in word if c.isalpha()))  # Apenas letras\n",
    "                .filter(lambda word: len(word) > 0))  # Remover strings vazias\n",
    "        \n",
    "        # Contagem de palavras\n",
    "        print(\"Contando palavras...\")\n",
    "        word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "        \n",
    "        # Coletar estat√≠sticas\n",
    "        total_words = words.count()\n",
    "        unique_words = word_counts.count()\n",
    "        \n",
    "        print(f\"\\nüìä Estat√≠sticas:\")\n",
    "        print(f\"Total de palavras processadas: {total_words:,}\")\n",
    "        print(f\"Palavras √∫nicas: {unique_words:,}\")\n",
    "        \n",
    "        # Mostrar as 15 palavras mais comuns\n",
    "        print(\"\\nüîù Top 15 palavras mais comuns:\")\n",
    "        top_words = word_counts.takeOrdered(15, key=lambda x: -x[1])\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word:<15} : {count:>6,}\")\n",
    "        \n",
    "        # Salvar resultado no HDFS\n",
    "        output_path = \"hdfs://spark-master:9000/datasets_processed/word_count_result\"\n",
    "        print(f\"\\nüíæ Salvando resultado em: {output_path}\")\n",
    "        \n",
    "        # Remover diret√≥rio se existir\n",
    "        try:\n",
    "            sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration()).delete(\n",
    "                sc._jvm.org.apache.hadoop.fs.Path(output_path), True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Salvar resultado\n",
    "        word_counts.saveAsTextFile(output_path)\n",
    "        print(\"‚úÖ Resultado salvo com sucesso no HDFS!\")\n",
    "        \n",
    "        # Verificar arquivos salvos\n",
    "        print(\"\\nüìÅ Verificando arquivos salvos:\")\n",
    "        saved_files = sc.textFile(output_path + \"/part-*\")\n",
    "        print(f\"Linhas salvas: {saved_files.count()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante o processamento: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Executar a an√°lise\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    if success:\n",
    "        print(\"\\nüéâ An√°lise conclu√≠da com sucesso!\")\n",
    "    else:\n",
    "        print(\"\\nüòû An√°lise falhou. Verifique os logs acima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39d527",
   "metadata": {},
   "source": [
    "## Verificando Resultado Salvo\n",
    "Vamos verificar o que foi salvo no HDFS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5206b",
   "metadata": {},
   "source": [
    "## Executando a contagem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ac0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando an√°lise de palavras...\n",
      "Iniciando an√°lise de palavras...\n",
      "Lendo arquivos do HDFS...\n",
      "Lendo arquivos: hdfs://spark-master:9000/datasets/*.txt\n",
      "N√∫mero de parti√ß√µes: 5\n",
      "Processando texto...\n",
      "Contando palavras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Estat√≠sticas:\n",
      "Total de palavras processadas: 614,956\n",
      "Palavras √∫nicas: 33,976\n",
      "\n",
      "üîù Top 15 palavras mais comuns:\n",
      "   1. the             : 36,279\n",
      "   2. and             : 21,402\n",
      "   3. that            : 10,899\n",
      "   4. was             :  9,166\n",
      "   5. his             :  8,155\n",
      "   6. her             :  7,340\n",
      "   7. you             :  7,174\n",
      "   8. with            :  6,804\n",
      "   9. not             :  6,489\n",
      "  10. had             :  6,323\n",
      "  11. for             :  6,026\n",
      "  12. but             :  5,830\n",
      "  13. she             :  5,726\n",
      "  14. have            :  4,254\n",
      "  15. him             :  4,168\n",
      "\n",
      "üíæ Salvando resultado em: hdfs://spark-master:9000/datasets_processed/word_count_result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultado salvo com sucesso no HDFS!\n",
      "\n",
      "üìÅ Verificando arquivos salvos:\n",
      "Linhas salvas: 33976\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o Spark est√° funcionando antes de executar\n",
    "if 'spark' in globals() and 'sc' in globals():\n",
    "    print(\"Executando an√°lise de palavras...\")\n",
    "    main()\n",
    "else:\n",
    "    print(\"‚ùå Spark n√£o est√° inicializado corretamente. Execute a c√©lula de inicializa√ß√£o primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759330a",
   "metadata": {},
   "source": [
    "## Finalizando Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5dea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Finalizar Spark de forma segura\n",
    "try:\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "        print(\"‚úÖ Spark finalizado com sucesso!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Nenhuma sess√£o Spark ativa encontrada\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro ao finalizar Spark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6e742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Verificando arquivos em: hdfs://spark-master:9000/datasets_processed/word_count_result\n",
      "‚ùå Erro ao verificar resultado: 'NoneType' object has no attribute 'sc'\n"
     ]
    }
   ],
   "source": [
    "# Verificar resultado salvo no HDFS\n",
    "try:\n",
    "    result_path = \"hdfs://spark-master:9000/datasets_processed/word_count_result\"\n",
    "    print(f\"üìÇ Verificando arquivos em: {result_path}\")\n",
    "    \n",
    "    # Ler resultado salvo\n",
    "    saved_results = sc.textFile(result_path + \"/part-*\")\n",
    "    \n",
    "    print(f\"\\nüìä Total de linhas no resultado: {saved_results.count()}\")\n",
    "    print(\"\\nüîç Primeiras 10 linhas do resultado:\")\n",
    "    \n",
    "    for i, line in enumerate(saved_results.take(10), 1):\n",
    "        print(f\"  {i:2d}. {line}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao verificar resultado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a297fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Finalizando Spark...\n",
      "‚úÖ SparkContext finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Finalizar SparkContext (boa pr√°tica)\n",
    "print(\"\\nüîÑ Finalizando Spark...\")\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ SparkContext finalizado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao finalizar: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
