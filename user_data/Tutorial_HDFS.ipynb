{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4b065a",
   "metadata": {},
   "source": [
    "# Tutorial: Como Usar HDFS no Cluster Hadoop/Spark\n",
    "\n",
    "Este tutorial explica como usar comandos HDFS para gerenciar arquivos no sistema distribuído do Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9abdc",
   "metadata": {},
   "source": [
    "## O que é HDFS?\n",
    "\n",
    "HDFS (Hadoop Distributed File System) é um sistema de arquivos distribuído que permite armazenar grandes volumes de dados em múltiplos nós de um cluster.\n",
    "\n",
    "### Características principais:\n",
    "- **Distribuído**: Os dados são espalhados por vários nós\n",
    "- **Tolerante a falhas**: Replica dados automaticamente\n",
    "- **Escalável**: Pode crescer adicionando mais nós"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975528ac",
   "metadata": {},
   "source": [
    "## Comandos HDFS Básicos\n",
    "\n",
    "Os comandos HDFS seguem o padrão: `hdfs dfs -<comando> <origem> <destino>`\n",
    "\n",
    "### Comandos mais utilizados:\n",
    "- `hdfs dfs -ls` - Lista arquivos e diretórios\n",
    "- `hdfs dfs -mkdir` - Cria diretórios\n",
    "- `hdfs dfs -put` - Copia arquivos do sistema local para HDFS\n",
    "- `hdfs dfs -get` - Copia arquivos do HDFS para sistema local\n",
    "- `hdfs dfs -cat` - Mostra conteúdo de um arquivo\n",
    "- `hdfs dfs -rm` - Remove arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae135eff",
   "metadata": {},
   "source": [
    "## Passo a Passo: Copiando Arquivos para HDFS\n",
    "\n",
    "### 1. Primeiro, vamos verificar se o HDFS está funcionando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5424e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute este comando no terminal do Jupyter\n",
    "# Para verificar se o HDFS está ativo\n",
    "!hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24af7a",
   "metadata": {},
   "source": [
    "### 2. Criar o diretório /datasets no HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5460fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o diretório datasets no HDFS\n",
    "!hdfs dfs -mkdir -p /datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebf2a1",
   "metadata": {},
   "source": [
    "### 3. Verificar se os arquivos do Gutenberg existem localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar os arquivos disponíveis na pasta gutenberg\n",
    "!ls -la /user_data/gutenberg/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fbc65",
   "metadata": {},
   "source": [
    "### 4. Copiar os arquivos .txt para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a2601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este é o comando do seu exercício!\n",
    "# Copia todos os arquivos .txt da pasta gutenberg para o diretório /datasets no HDFS\n",
    "!hdfs dfs -put /user_data/gutenberg/*.txt /datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabadd4",
   "metadata": {},
   "source": [
    "### 5. Verificar se os arquivos foram copiados com sucesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858490e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar os arquivos no diretório /datasets do HDFS\n",
    "!hdfs dfs -ls /datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99506255",
   "metadata": {},
   "source": [
    "### 6. Verificar o tamanho e detalhes dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar informações detalhadas dos arquivos\n",
    "!hdfs dfs -ls -h /datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5dc6c",
   "metadata": {},
   "source": [
    "## Explicação do Comando do Exercício\n",
    "\n",
    "```bash\n",
    "hdfs dfs -put /user_data/gutenberg/*.txt /datasets/\n",
    "```\n",
    "\n",
    "### Quebrando o comando:\n",
    "- `hdfs dfs`: Interface de linha de comando para HDFS\n",
    "- `-put`: Comando para copiar arquivos do sistema local para HDFS\n",
    "- `/user_data/gutenberg/*.txt`: Origem - todos os arquivos .txt na pasta gutenberg\n",
    "- `/datasets/`: Destino - diretório no HDFS onde os arquivos serão armazenados\n",
    "\n",
    "### O que acontece:\n",
    "1. O Hadoop pega todos os arquivos .txt da pasta local `/user_data/gutenberg/`\n",
    "2. Copia esses arquivos para o sistema distribuído HDFS\n",
    "3. Os arquivos ficam disponíveis em `/datasets/` no HDFS\n",
    "4. Agora você pode processar esses dados com Spark de forma distribuída"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b04729",
   "metadata": {},
   "source": [
    "## Comandos Úteis Adicionais\n",
    "\n",
    "### Ver conteúdo de um arquivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca18f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar as primeiras linhas de um arquivo\n",
    "!hdfs dfs -cat /datasets/Romeo_and_Juliet.txt | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bec2be",
   "metadata": {},
   "source": [
    "### Verificar o espaço usado no HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392fae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar estatísticas de uso do HDFS\n",
    "!hdfs dfs -df -h /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d1f7e",
   "metadata": {},
   "source": [
    "## Próximos Passos\n",
    "\n",
    "Depois de copiar os arquivos para o HDFS, você pode:\n",
    "1. Usar Spark para processar esses dados\n",
    "2. Executar análises de big data\n",
    "3. Criar DataFrames do Spark a partir desses arquivos\n",
    "4. Executar operações MapReduce\n",
    "\n",
    "Os dados agora estão prontos para processamento distribuído!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
