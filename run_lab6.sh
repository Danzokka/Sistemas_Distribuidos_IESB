#!/bin/bash

# Script para executar o Laborat√≥rio 6 - An√°lise de Texto com PySpark e API
# Sistemas Distribu√≠dos - IESB

echo "=== Laborat√≥rio 6: An√°lise de Texto com PySpark e API ==="
echo ""

# Fun√ß√£o para verificar se o comando foi executado com sucesso
check_status() {
    if [ $? -eq 0 ]; then
        echo "‚úÖ $1"
    else
        echo "‚ùå Erro em: $1"
        exit 1
    fi
}

# Fun√ß√£o para aguardar o servi√ßo estar pronto
wait_for_service() {
    local service_name=$1
    local port=$2
    local max_attempts=30
    local attempt=0
    
    echo "Aguardando $service_name na porta $port..."
    while [ $attempt -lt $max_attempts ]; do
        if curl -s -f http://localhost:$port > /dev/null 2>&1; then
            echo "‚úÖ $service_name est√° pronto!"
            return 0
        fi
        attempt=$((attempt + 1))
        sleep 2
    done
    echo "‚ö†Ô∏è  $service_name pode n√£o estar totalmente pronto, mas continuando..."
}

echo "1. Parando ambiente Docker se estiver rodando..."
sudo docker compose down
check_status "Ambiente Docker parado"

echo ""
echo "2. Construindo as imagens Docker..."
sudo docker compose build
check_status "Imagens Docker constru√≠das"

echo ""
echo "3. Iniciando o ambiente Docker..."
sudo docker compose up -d
check_status "Ambiente Docker iniciado"

echo ""
echo "4. Aguardando inicializa√ß√£o dos servi√ßos (60 segundos)..."
sleep 60

echo ""
echo "5. Verificando se os containers est√£o rodando..."
sudo docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo ""
echo "6. Copiando arquivos do Gutenberg para o HDFS..."
# Aguardar o HDFS estar pronto
echo "Aguardando HDFS estar pronto..."
sleep 30

# Executar o comando de c√≥pia para o HDFS
sudo docker exec -it spark-master bash -c "cd /user_data/gutenberg && hdfs dfs -put *.txt /datasets/" 2>/dev/null
if [ $? -eq 0 ]; then
    echo "‚úÖ Arquivos copiados para o HDFS"
else
    echo "‚ö†Ô∏è  Tentando novamente em 15 segundos..."
    sleep 15
    sudo docker exec -it spark-master bash -c "cd /user_data/gutenberg && hdfs dfs -put *.txt /datasets/" 2>/dev/null
    check_status "Arquivos copiados para o HDFS (segunda tentativa)"
fi

echo ""
echo "7. Verificando arquivos no HDFS..."
sudo docker exec -it spark-master hdfs dfs -ls /datasets/
check_status "Verifica√ß√£o do HDFS"

echo ""
echo "8. Aguardando API estar pronta..."
wait_for_service "API" 8001

echo ""
echo "9. Testando a API..."
echo "Testando endpoint principal:"
curl -s http://localhost:8001/ | python3 -m json.tool 2>/dev/null || curl -s http://localhost:8001/
echo ""
echo "Testando endpoint do microservi√ßo:"
curl -s http://localhost:8001/micro_servico | python3 -m json.tool 2>/dev/null || curl -s http://localhost:8001/micro_servico
echo ""

echo ""
echo "=== LABORAT√ìRIO 6 CONFIGURADO COM SUCESSO! ==="
echo ""
echo "üìä Servi√ßos dispon√≠veis:"
echo "   ‚Ä¢ Spark Master UI: http://localhost:8080"
echo "   ‚Ä¢ Jupyter Notebook: http://localhost:8888"
echo "   ‚Ä¢ HDFS NameNode UI: http://localhost:9870"
echo "   ‚Ä¢ YARN ResourceManager: http://localhost:8088"
echo "   ‚Ä¢ API FastAPI: http://localhost:8001"
echo "   ‚Ä¢ API Swagger UI: http://localhost:8001/docs"
echo ""
echo "üìù Pr√≥ximos passos:"
echo "   1. Acesse o Jupyter Notebook em http://localhost:8888"
echo "   2. Abra o notebook 'contar_palavras.ipynb'"
echo "   3. Execute as c√©lulas para fazer a an√°lise de palavras"
echo "   4. Teste a API em http://localhost:8001/docs"
echo ""
echo "üîß Para parar o ambiente: sudo docker compose down"